{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTCU9cu_pvIi",
        "outputId": "fe3c4c15-5d65-40cc-8661-0bc58d08c974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper\n",
        "!apt-get install ffmpeg"
      ],
      "metadata": {
        "id": "DHbjZ_EKp9x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'device: {device}')\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"medium\", device=device)  # You can use \"small\", \"medium\", or \"large\" for better accuracy\n",
        "\n",
        "# Path to your video file in Google Drive\n",
        "video_path = \"path_to_file\"\n",
        "print(f\"Transcribing: {video_path}\")\n",
        "\n",
        "# Transcribe the video\n",
        "result = model.transcribe(video_path, language=\"English\", task=\"transcribe\")\n",
        "\n",
        "# Collect each segment's text on a new line\n",
        "lines = [segment['text'].strip() for segment in result['segments']]\n",
        "formatted_text = '\\n'.join(lines)\n",
        "\n",
        "# Output path for the transcript file\n",
        "output_path = os.path.splitext(video_path)[0] + '.txt'\n",
        "\n",
        "# Save the formatted transcript\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(formatted_text)\n",
        "\n",
        "print(f\"Transcript saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf1p1zMtHsdp",
        "outputId": "c67b0b3b-287e-413d-c32c-628c9b80ae6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 1.42G/1.42G [00:14<00:00, 106MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/videos/2025-04-18 11-05-49.mkv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/videos/2025-04-18 11-05-49.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Check if CUDA (GPU support) is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f'device: {device}')\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"medium\", device=device)  # \"small\", \"medium\", or \"large\"\n",
        "\n",
        "# Folder containing videos\n",
        "folder_path = 'path_to_folder'\n",
        "\n",
        "# Process each video in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.lower().endswith(('.mp4', '.mkv', '.avi', '.mov')):\n",
        "        video_path = os.path.join(folder_path, filename)\n",
        "        print(f\"Transcribing: {video_path}\")\n",
        "\n",
        "        # Transcribe the video\n",
        "        result = model.transcribe(video_path, language=\"English\", task=\"transcribe\")\n",
        "\n",
        "        # Collect each segment's text on a new line\n",
        "        lines = [segment['text'].strip() for segment in result['segments']]\n",
        "        formatted_text = '\\n'.join(lines)\n",
        "\n",
        "        # Output path for the transcript file\n",
        "        output_path = os.path.splitext(video_path)[0] + '.txt'\n",
        "\n",
        "        # Save the formatted transcript\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(formatted_text)\n",
        "\n",
        "        print(f\"Transcript saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqNg1ZZcBMb6",
        "outputId": "eb122d3c-307e-49e3-c1f1-051d4065deff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L10 - 2025-02-11 12-10-11.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L10 - 2025-02-11 12-10-11.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L23 - 2025-03-09 11-02-42.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L23 - 2025-03-09 11-02-42.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L05 - 2025-02-03 08-38-23.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L05 - 2025-02-03 08-38-23.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L06 - 2025-02-04 12-00-15.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L06 - 2025-02-04 12-00-15.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L07 - 2025-02-07 10-14-54.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L07 - 2025-02-07 10-14-54.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L08 - 2025-02-08 12-02-22.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L08 - 2025-02-08 12-02-22.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L09 - 2025-02-10 08-06-04.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L09 - 2025-02-10 08-06-04.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L18 - 2025-02-25 12-03-03.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L18 - 2025-02-25 12-03-03.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L19 - 2025-03-03 08-02-34.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L19 - 2025-03-03 08-02-34.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L20 - 2025-03-04 12-03-46.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L20 - 2025-03-04 12-03-46.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L21 - 2025-03-06 11-03-45.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L21 - 2025-03-06 11-03-45.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L22 - 2025-03-07 10-03-41.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L22 - 2025-03-07 10-03-41.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L24 - 2025-03-10 08-07-47.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L24 - 2025-03-10 08-07-47.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L25 - 2025-03-11 12-11-00.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L25 - 2025-03-11 12-11-00.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L16 - 2025-02-21 10-04-51.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L16 - 2025-02-21 10-04-51.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L12 - 2025-02-14 10-01-58.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L12 - 2025-02-14 10-01-58.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L13 - 2025-02-17 08-05-37.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L13 - 2025-02-17 08-05-37.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L15 - 2025-02-20 11-04-04.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L15 - 2025-02-20 11-04-04.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L14 - 2025-02-18 12-02-48.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L14 - 2025-02-18 12-02-48.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L11 - 2025-02-13 08-05-21.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L11 - 2025-02-13 08-05-21.txt\n",
            "Transcribing: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L17 - 2025-02-24 12-04-22.mkv\n",
            "Transcript saved to: /content/drive/MyDrive/iitm/a_iitm_nlp/video_before_midsem/L17 - 2025-02-24 12-04-22.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1MW61TdvqsjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afcjKIvGrCQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}